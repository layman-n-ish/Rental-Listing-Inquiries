{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-5240fdcbee4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mcount_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mcount_features_train_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mcount_features_test_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[1;32m    120\u001b[0m                              \"unicode string.\")\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train = pd.read_csv(\"data/train_all_feat.csv\")\n",
    "test  = pd.read_csv(\"data/test_all_feat.csv\")\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = pd.read_csv(\"data/y.csv\")\n",
    "train_y = np.array(train_y['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "features_to_use=[\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\", \"num_features\", \n",
    "    \"len_description\",\"created_year\", \"created_month\", \"created_day\", \"created_hour\", \"created_weekday\", \n",
    "    \"is_night\", \"is_weekend\", \"price_per_bed\", \"price_per_bath\", \"total_rooms\", \"price_per_room\",\n",
    "    \"manager_skill\", \"building_popularity\", \"logprice\", \"density\"]\n",
    "\n",
    "count_features = CountVectorizer(stop_words='english', max_features=500,ngram_range=(1,4))\n",
    "count_features_train_sparse = count_features.fit_transform(train[\"features\"])\n",
    "count_features_test_sparse = count_features.transform(test[\"features\"])\n",
    "\n",
    "train_X = sparse.hstack([train[features_to_use], count_features_train_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test[features_to_use], count_features_test_sparse]).tocsr()\n",
    "\n",
    "tfidf_features = TfidfVectorizer(stop_words='english', max_features=500,ngram_range=(1,4))\n",
    "tfidf_features_train_sparse = count_features.fit_transform(train_df[\"features\"])\n",
    "tfidf_features_test_sparse = count_features.transform(test_df[\"features\"])\n",
    "\n",
    "train_X = sparse.hstack([train_X, tfidf_features_train_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_X, tfidf_features_test_sparse]).tocsr()\n",
    "\n",
    "count_desc = CountVectorizer(stop_words='english', max_features=500,ngram_range=(1,4))\n",
    "count_desc_train_sparse = count_desc.fit_transform(train_df[\"description\"])\n",
    "count_desc_test_sparse = count_desc.transform(test_df[\"description\"])\n",
    "\n",
    "train_X = sparse.hstack([train_X, count_desc_train_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_X, count_desc_test_sparse]).tocsr()\n",
    "\n",
    "tfidf_desc = CountVectorizer(stop_words='english', max_features=500,ngram_range=(1,4))\n",
    "tfidf_desc_train_sparse = tfidf_desc.fit_transform(train_df[\"description\"])\n",
    "tfidf_desc_test_sparse = tfidf_desc.transform(test_df[\"description\"])\n",
    "\n",
    "train_X = sparse.hstack([train_X, tfidf_desc_train_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_X, tfidf_desc_test_sparse]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bathrooms  bedrooms  latitude  longitude   price  num_features  \\\n",
      "0        1.5         3   40.7145   -73.9425  3000.0           0.0   \n",
      "1        1.0         1   40.7388   -74.0018  2850.0           4.0   \n",
      "2        1.0         1   40.7539   -73.9677  3275.0           2.0   \n",
      "3        2.0         4   40.7429   -74.0028  7995.0           0.0   \n",
      "4        1.0         1   40.8234   -73.9457  1725.0           4.0   \n",
      "\n",
      "   len_description  created_year  created_month  created_day  \\\n",
      "0            561.0          2016              6           24   \n",
      "1            657.0          2016              4           17   \n",
      "2            404.0          2016              4           18   \n",
      "3              8.0          2016              4           19   \n",
      "4             22.0          2016              4           20   \n",
      "\n",
      "             ...             price_per_bed  price_per_bath  total_rooms  \\\n",
      "0            ...                     750.0          1200.0          4.5   \n",
      "1            ...                    1425.0          1425.0          2.0   \n",
      "2            ...                    1637.5          1637.5          2.0   \n",
      "3            ...                    1599.0          2665.0          6.0   \n",
      "4            ...                     862.5           862.5          2.0   \n",
      "\n",
      "   price_per_room  manager_skill  building_popularity  manager_listings_count  \\\n",
      "0      545.454545       0.264706             0.500000                    90.0   \n",
      "1      950.000000       0.504854             0.750000                   134.0   \n",
      "2     1091.666667       0.253012             0.107143                   191.0   \n",
      "3     1142.142857       0.120253             0.538462                   210.0   \n",
      "4      575.000000       0.652778             1.100000                    86.0   \n",
      "\n",
      "   building_listings_count  s_address_listings_count  d_address_listings_count  \n",
      "0                      3.0                       3.0                      22.0  \n",
      "1                     58.0                      30.0                      69.0  \n",
      "2                     99.0                      57.0                      73.0  \n",
      "3                     13.0                       9.0                      59.0  \n",
      "4                     25.0                      23.0                      23.0  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "['medium' 'high' 'low' ... 'low' 'low' 'low']\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv('data/X.csv')\n",
    "y = np.asarray(pd.read_csv('data/y.csv')).ravel()\n",
    "\n",
    "X = X.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "print(X.head())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_impurity_split': None, 'n_jobs': 1, 'n_estimators': 10, 'min_samples_leaf': 1, 'bootstrap': True, 'max_depth': None, 'warm_start': False, 'oob_score': False, 'verbose': 0, 'random_state': None, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'class_weight': None, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'criterion': 'gini', 'max_features': 'auto'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "print(model.get_params())\n",
    "\n",
    "grid = {'n_estimators' : [200, 350, 500, 750, 1000],\n",
    "        'max_depth': [int(x) for x in np.linspace(10, 110, 11)],\n",
    "        'min_samples_split': [2, 5, 10, 25, 50],\n",
    "        'min_samples_leaf': [2, 5, 10, 25, 50],\n",
    "        }\n",
    "#'max_feaures': ['sqrt', 'log2']\n",
    "#'bootstrap': [True, False]\n",
    "\n",
    "kFold = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n",
    "randomSearch = RandomizedSearchCV(estimator=model, param_distributions=grid, n_iter=50, \n",
    "                                  verbose=2, scoring=\"neg_log_loss\", n_jobs=-1, cv=kFold)\n",
    "\n",
    "rs_result = randomSearch.fit(X, y)\n",
    "\n",
    "print(\"Best: %f using %s\" % (rs_result.best_score_, rs_result.best_params_))\n",
    "means = rs_result.cv_results_['mean_test_score']\n",
    "stds = rs_result.cv_results_['std_test_score']\n",
    "params = rs_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[CV] min_samples_split=10, max_depth=10, n_estimators=1000, min_samples_leaf=5 \n",
      "[CV] min_samples_split=10, max_depth=10, n_estimators=1000, min_samples_leaf=5 \n",
      "[CV] min_samples_split=10, max_depth=10, n_estimators=1000, min_samples_leaf=5 \n",
      "[CV] min_samples_split=2, max_depth=110, n_estimators=750, min_samples_leaf=2 \n",
      "[CV]  min_samples_split=10, max_depth=10, n_estimators=1000, min_samples_leaf=5, total= 1.0min\n",
      "[CV] min_samples_split=2, max_depth=110, n_estimators=750, min_samples_leaf=2 \n",
      "[CV]  min_samples_split=10, max_depth=10, n_estimators=1000, min_samples_leaf=5, total= 1.0min\n",
      "[CV] min_samples_split=2, max_depth=110, n_estimators=750, min_samples_leaf=2 \n",
      "[CV]  min_samples_split=10, max_depth=10, n_estimators=1000, min_samples_leaf=5, total= 1.0min\n",
      "[CV] min_samples_split=50, max_depth=10, n_estimators=500, min_samples_leaf=2 \n",
      "[CV]  min_samples_split=2, max_depth=110, n_estimators=750, min_samples_leaf=2, total= 1.0min\n",
      "[CV] min_samples_split=50, max_depth=10, n_estimators=500, min_samples_leaf=2 \n",
      "[CV]  min_samples_split=50, max_depth=10, n_estimators=500, min_samples_leaf=2, total=  27.3s\n",
      "[CV] min_samples_split=50, max_depth=10, n_estimators=500, min_samples_leaf=2 \n",
      "[CV]  min_samples_split=50, max_depth=10, n_estimators=500, min_samples_leaf=2, total=  27.5s\n",
      "[CV] min_samples_split=5, max_depth=50, n_estimators=200, min_samples_leaf=25 \n",
      "[CV]  min_samples_split=5, max_depth=50, n_estimators=200, min_samples_leaf=25, total=  11.8s\n",
      "[CV] min_samples_split=5, max_depth=50, n_estimators=200, min_samples_leaf=25 \n",
      "[CV]  min_samples_split=5, max_depth=50, n_estimators=200, min_samples_leaf=25, total=  12.1s\n",
      "[CV] min_samples_split=5, max_depth=50, n_estimators=200, min_samples_leaf=25 \n",
      "[CV]  min_samples_split=50, max_depth=10, n_estimators=500, min_samples_leaf=2, total=  26.8s\n",
      "[CV] min_samples_split=2, max_depth=70, n_estimators=1000, min_samples_leaf=10 \n",
      "[CV]  min_samples_split=2, max_depth=110, n_estimators=750, min_samples_leaf=2, total= 1.0min\n",
      "[CV] min_samples_split=2, max_depth=70, n_estimators=1000, min_samples_leaf=10 \n",
      "[CV]  min_samples_split=2, max_depth=110, n_estimators=750, min_samples_leaf=2, total= 1.0min\n",
      "[CV] min_samples_split=2, max_depth=70, n_estimators=1000, min_samples_leaf=10 \n",
      "[CV]  min_samples_split=5, max_depth=50, n_estimators=200, min_samples_leaf=25, total=  11.8s\n",
      "[CV] min_samples_split=5, max_depth=80, n_estimators=200, min_samples_leaf=25 \n",
      "[CV]  min_samples_split=5, max_depth=80, n_estimators=200, min_samples_leaf=25, total=  11.8s\n",
      "[CV] min_samples_split=5, max_depth=80, n_estimators=200, min_samples_leaf=25 \n",
      "[CV]  min_samples_split=5, max_depth=80, n_estimators=200, min_samples_leaf=25, total=  11.8s\n",
      "[CV] min_samples_split=5, max_depth=80, n_estimators=200, min_samples_leaf=25 \n",
      "[CV]  min_samples_split=5, max_depth=80, n_estimators=200, min_samples_leaf=25, total=  11.8s\n",
      "[CV] min_samples_split=5, max_depth=90, n_estimators=500, min_samples_leaf=50 \n",
      "[CV]  min_samples_split=2, max_depth=70, n_estimators=1000, min_samples_leaf=10, total= 1.1min\n",
      "[CV] min_samples_split=5, max_depth=90, n_estimators=500, min_samples_leaf=50 \n",
      "[CV]  min_samples_split=5, max_depth=90, n_estimators=500, min_samples_leaf=50, total=  26.2s\n",
      "[CV] min_samples_split=5, max_depth=90, n_estimators=500, min_samples_leaf=50 \n",
      "[CV]  min_samples_split=2, max_depth=70, n_estimators=1000, min_samples_leaf=10, total= 1.1min\n",
      "[CV] min_samples_split=25, max_depth=60, n_estimators=1000, min_samples_leaf=50 \n",
      "[CV]  min_samples_split=2, max_depth=70, n_estimators=1000, min_samples_leaf=10, total= 1.1min\n",
      "[CV] min_samples_split=25, max_depth=60, n_estimators=1000, min_samples_leaf=50 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-27d58934b7ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                   verbose=2, scoring=\"neg_log_loss\", n_jobs=-1, cv=kFold)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrs_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomSearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrs_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
